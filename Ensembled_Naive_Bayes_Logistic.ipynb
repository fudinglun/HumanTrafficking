{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#load needed python library\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import matthews_corrcoef,recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanfangwang/anaconda3/envs/nlp11/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (25,27,40,41,43,44,45,46,47,75,76,78,79,87,89,90,91,92,110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Read in the data file\n",
    "dt = pd.read_csv(\"all_ter_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove the nulls, provider_id, reviewer_id\n",
    "dt = dt[~pd.isnull(dt.provider_id)]\n",
    "#fill in zeros for the null in ht column\n",
    "dt['ht'].fillna(0,inplace = True)\n",
    "dt['ht'] = dt['ht'].astype('bool')\n",
    "v = dt[['provider_id','general_details','juicy_details','ht']]\n",
    "del dt\n",
    "#fill in the Na's in general and juicy details to be ''\n",
    "v['general_details'].fillna(\"\",inplace = True)\n",
    "v['juicy_details'].fillna(\"\",inplace = True)\n",
    "#vectorizer using binary \n",
    "stopWords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aggreagate the reviews based on provider_id\n",
    "pid, comments, label = [], [], []\n",
    "for name, group in v.groupby(\"provider_id\"):\n",
    "    pid.append(name)\n",
    "    label.append(any(group.ht))\n",
    "    compound = group.general_details + group.juicy_details\n",
    "    comments.append(\" \".join(compound))\n",
    "label = np.array(label) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vectorizer the comments [so that it could be put into machine learning models] \n",
    "# could change setting, for examples, ngram_range = (1,2) allows bi-grams to be used\n",
    "vect_count = CountVectorizer(ngram_range = (1,1), binary = False, stop_words = list(stopWords)) \n",
    "sp_mat = vect_count.fit_transform(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up the cross validation procedure\n",
    "ss_index = []\n",
    "for i,j in StratifiedKFold(label,4):\n",
    "    ss_index.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###The ensemble models \n",
    "#create the bootstrape index and subsampling index for our data\n",
    "def boot_model(trainX,trainY,repeatn,prop = 1, bootp = None):\n",
    "    Xsize = trainX.shape[0]\n",
    "    p_index = trainY.index[trainY == 1]\n",
    "    n_index = trainY.index[trainY == 0]\n",
    "    psize, nsize = p_index.size, n_index.size\n",
    "    boot_ind = np.zeros((repeatn,psize+int(psize*prop)))\n",
    "    for i in range(repeatn):\n",
    "        if bootp == None:\n",
    "            pb = np.random.choice(p_index,psize)\n",
    "        else:\n",
    "            pb = np.random.choice(p_index,int(psize*bootp),replace = False)\n",
    "            pb = np.concatenate((pb,np.random.choice(pb,psize-pb.size)))\n",
    "        nb = np.random.choice(n_index,int(psize*prop),replace = False)\n",
    "        pb = np.concatenate((pb,nb))\n",
    "        boot_ind[i,:] = np.random.permutation(pb)\n",
    "    return boot_ind\n",
    " \n",
    "    \n",
    "#model the predefined sklearn models, logistics, svm, naive bayes, etc.\n",
    "#prop describes how much to subsample from class 0 / #of class 1\n",
    "def cross_val_boot(X,Y,n_splits,n_repeat,model, prop = 1, bootp = None):\n",
    "    #rs = StratifiedShuffleSplit(n_splits = n_splits)\n",
    "    r_result = []\n",
    "    for train_ind, test_ind in ss_index:\n",
    "        trainX, testX = X[train_ind,:], X[test_ind,:]\n",
    "        trainY, testY = Y[train_ind], Y[test_ind]\n",
    "        boot_ind = boot_model(trainX,trainY,n_repeat,prop)\n",
    "        pre_testY = np.zeros((n_repeat,testX.shape[0]))\n",
    "        pre_testY_auc = np.zeros((n_repeat,testX.shape[0]))\n",
    "        for i in range(n_repeat):\n",
    "            bd = boot_ind[i,:]\n",
    "            bootX, bootY = X[bd,:], trainY[bd]\n",
    "            try:\n",
    "                model.fit(bootX,bootY)\n",
    "            except:\n",
    "                print(bootY)\n",
    "            pre_testY[i,:] = model.predict(testX)\n",
    "            pre_testY_auc[i,:] = model.predict_proba(testX)[:,1]\n",
    "        preY = pre_testY.sum(axis = 0) > (n_repeat//2)\n",
    "        pre_testY_auc = pre_testY_auc.max(axis = 0)\n",
    "        accuracy = sum(preY == testY)/len(testY)\n",
    "        fpr, tpr, thresholds = roc_curve(testY, pre_testY_auc)\n",
    "        s = auc(fpr, tpr)\n",
    "        r_result.append((accuracy, precision_score(testY,preY),recall_score(testY,preY),matthews_corrcoef(testY,preY),s))\n",
    "    r_result = np.array(r_result).reshape((n_splits,-1))\n",
    "    return r_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize models\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model1 = MultinomialNB()\n",
    "model2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evalidation on the naive bayes ensemble, using 10 models per fold (could adjust the parameter)\n",
    "cross_val_boot(sp_mat,pd.Series(label),4,10,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluation on the logistic regression ensemble, using 10 models per fold (could adjust the parameter)\n",
    "cross_val_boot(sp_mat,pd.Series(label),4,10,model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp11]",
   "language": "python",
   "name": "conda-env-nlp11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
